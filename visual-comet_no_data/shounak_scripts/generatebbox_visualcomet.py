# -*- coding: utf-8 -*-
"""generatebbox_visualcomet

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ez-bT8imzpd73QjSMq7gALN_zrEkM3vU

# Detectron2 Beginner's Tutorial

<img src="https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png" width="500">

Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of detectron2, including the following:
* Run inference on images or videos, with an existing detectron2 model
* Train a detectron2 model on a new dataset

You can make a copy of this tutorial by "File -> Open in playground mode" and make changes there. __DO NOT__ request access to this tutorial.

# Install detectron2
"""

#!python -m pip install pyyaml==5.1
# Detectron2 has not released pre-built binaries for the latest pytorch (https://github.com/facebookresearch/detectron2/issues/4053)
# so we install from source instead. This takes a few minutes.
#!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

# Install pre-built detectron2 that matches pytorch version, if released:
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
#!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/{CUDA_VERSION}/{TORCH_VERSION}/index.html

# exit(0)  # After installation, you may need to "restart runtime" in Colab. This line can also restart runtime

import torch, detectron2
#!nvcc --version
TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import os, json, cv2, random
#from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog

"""# Run a pre-trained detectron2 model

We first download an image from the COCO dataset:
"""

#!wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg

import glob
files = glob.glob("/home/shounak_rtml/11777/visual-comet/images2/social_iq/all/*.jpg")
print(files)

# im = cv2.imread("/content/drive/MyDrive/11777_share/_180.png")
# cv2_imshow(im)

"""Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."""

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)


import cv2 as cv

# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification
#print(outputs["instances"].pred_classes)
#print(outputs["instances"].pred_boxes)
#print(outputs["instances"].pred_masks)
#print(im.shape)
#https://github.com/facebookresearch/detectron2/issues/2245
import matplotlib.pyplot as plt
import matplotlib.cm as cm

segms=[]
boxes=[]
height=[]
width=[]
names=[]


from PIL import Image as im

def close_contour(contour):
  if not np.array_equal(contour[0], contour[-1]):
    contour = np.vstack((contour, contour[0]))
  return contour

from tqdm import tqdm

exceptions=[]

for item in tqdm(files):
  try:
    image = cv2.imread(item)
    outputs = predictor(image)
    this_height=image.shape[0]
    this_width=image.shape[1]
    #names=outputs["instances"].pred_classes
    #print(outputs["instances"].pred_boxes[0])
    this_boxes=(outputs['instances'].pred_boxes.tensor.cpu().numpy()).tolist()
    #this_boxes=np.array(outputs["instances"].pred_boxes.cpu())
    #print(this_boxes)
    #print(height,width)

    pred_classes = outputs['instances'].pred_classes.cpu().tolist()
    class_names = MetadataCatalog.get("coco_2014_train").thing_classes
    pred_class_names = list(map(lambda x: class_names[x], pred_classes))
    this_name=pred_class_names
    #print(this_name)

    this_segm=[]

    mask_array = outputs['instances'].pred_masks.cpu().numpy()
    scores = outputs['instances'].scores.cpu().numpy()
    num_masks=mask_array.shape[0]
    num_masks2=len(this_name)
    tolerance=0
    #print(num_masks)
    
    for i in range(num_masks2):
      this_mask=mask_array[i]
      #print(this_mask)
      #bounds=np.where(this_mask == True)
      #bounds_x=bounds[0]
      #bounds_y=bounds[1]
      #print(bounds_x.shape)
      #fg
      #print(this_mask)
      this_mask = np.array(this_mask*255)
      #print(this_mask.shape)
      #print(this_mask)
      #polygons = []
      plt.imsave('filename.png', this_mask, cmap=cm.gray)
        
      # saving the final output 
      # as a PNG file
      im=cv.imread('filename.png', cv2.IMREAD_GRAYSCALE)

      _, thresh = cv2.threshold(im, 128,255,cv2.THRESH_BINARY)
      contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)
      maxlength = max(map(len, contours))

      a=np.squeeze(contours[0])
      #print((a))
      a=a.tolist()
      if(any(isinstance(i, list) for i in a) is False):
        #print("OK")
        a=[a]
      a=[a]
      #print("AA",a)
      #print(len(a))
      
      this_segm.append(a)
    #this_segm=this_segm[0:len()]

    #this_segm=[this_segm]
  
    #print(item)  
  
  
    #segms.append(this_segm)
    #print(this_segm)

    # num_instances = mask_array.shape[0]
    # mask_array = np.moveaxis(mask_array, 0, -1)
    # mask_array_instance = []
    # output = np.zeros_like(im) #black
    # #print('output',output)
    # for i in range(num_instances):
    #     mask_array_instance.append(mask_array[:, :, i:(i+1)])
    #     output = np.where(mask_array_instance[i] == True, 255, output)
    # cv2.imwrite(mask_path+'/'+item+'.jpg',output)#mask

    this_boxes2=np.array(this_boxes)
    #this_boxes2=np.squeeze(this_boxes2)
    #print(this_boxes2.shape,scores.shape)
    scores=np.reshape(scores,(len(scores),1))
    #print(scores.shape)
    #print(this_boxes2.shape)
    #this_boxes2=np.reshape(this_boxes2,(len(scores),1))
    this_boxes2=np.concatenate((this_boxes2,scores),axis=1)
    
    #print(this_boxes2.shape)
    this_boxes=this_boxes2.tolist()

    #print(this_boxes)
    
    segms.append(this_segm)
    boxes.append(this_boxes)
    height.append(this_height)
    width.append(this_width)
    names.append(this_name)
  except:
    exceptions.append(item)
#print(segms)

# coco_classes={0: u'__background__',
#  1: u'person',
#  2: u'bicycle',
#  3: u'car',
#  4: u'motorcycle',
#  5: u'airplane',
#  6: u'bus',
#  7: u'train',
#  8: u'truck',
#  9: u'boat',
#  10: u'traffic light',
#  11: u'fire hydrant',
#  12: u'stop sign',
#  13: u'parking meter',
#  14: u'bench',
#  15: u'bird',
#  16: u'cat',
#  17: u'dog',
#  18: u'horse',
#  19: u'sheep',
#  20: u'cow',
#  21: u'elephant',
#  22: u'bear',
#  23: u'zebra',
#  24: u'giraffe',
#  25: u'backpack',
#  26: u'umbrella',
#  27: u'handbag',
#  28: u'tie',
#  29: u'suitcase',
#  30: u'frisbee',
#  31: u'skis',
#  32: u'snowboard',
#  33: u'sports ball',
#  34: u'kite',
#  35: u'baseball bat',
#  36: u'baseball glove',
#  37: u'skateboard',
#  38: u'surfboard',
#  39: u'tennis racket',
#  40: u'bottle',
#  41: u'wine glass',
#  42: u'cup',
#  43: u'fork',
#  44: u'knife',
#  45: u'spoon',
#  46: u'bowl',
#  47: u'banana',
#  48: u'apple',
#  49: u'sandwich',
#  50: u'orange',
#  51: u'broccoli',
#  52: u'carrot',
#  53: u'hot dog',
#  54: u'pizza',
#  55: u'donut',
#  56: u'cake',
#  57: u'chair',
#  58: u'couch',
#  59: u'potted plant',
#  60: u'bed',
#  61: u'dining table',
#  62: u'toilet',
#  63: u'tv',
#  64: u'laptop',
#  65: u'mouse',
#  66: u'remote',
#  67: u'keyboard',
#  68: u'cell phone',
#  69: u'microwave',
#  70: u'oven',
#  71: u'toaster',
#  72: u'sink',
#  73: u'refrigerator',
#  74: u'book',
#  75: u'clock',
#  76: u'vase',
#  77: u'scissors',
#  78: u'teddy bear',
#  79: u'hair drier',
#  80: u'toothbrush'}

# coco_classes[65]

# names=[]
# for i in range(len(outputs["instances"].pred_classes)):
#   names.append(coco_classes[int(outputs["instances"].pred_classes[i])])
# print(names)

# # We can use `Visualizer` to draw the predictions on the image.
# v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
# out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
# cv2_imshow(out.get_image()[:, :, ::-1])

"""# Train on a custom dataset

In this section, we show how to train an existing detectron2 model on a custom dataset in a new format.

We use [the balloon segmentation dataset](https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon)
which only has one class: balloon.
We'll train a balloon segmentation model from an existing model pre-trained on COCO dataset, available in detectron2's model zoo.

Note that COCO dataset does not have the "balloon" category. We'll be able to recognize this new class in a few minutes.

## Prepare the dataset

Register the balloon dataset to detectron2, following the [detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).
Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into detectron2's standard format. User should write such a function when using a dataset in custom format. See the tutorial for more details.

To verify the dataset is in correct format, let's visualize the annotations of randomly selected samples in the training set:

# Run panoptic segmentation on a video
"""

#from google.colab import drive
#drive.mount('/content/drive')

import json
import os

details = []

for i in range(len(files)):
  try:
      this_dict={"boxes":boxes[i],
                 "segms":segms[i],
                 "names":names[i],
                 "width":width[i],
                 "height":height[i],
                 }
      details.append(this_dict)
      with open('/home/shounak_rtml/11777/visual-comet/images2/social_iq/all/'+os.path.basename(files[i]).split('.')[0]+'.json', 'w') as convert_file:
         convert_file.write(json.dumps(this_dict))
  except:
      print("issue")

#print(details)
print(exceptions)

